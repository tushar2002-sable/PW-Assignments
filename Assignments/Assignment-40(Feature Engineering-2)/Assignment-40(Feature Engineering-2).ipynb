{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba8520f",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide  an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28860f02",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to transform the values of a dataset's features into a specific range, usually [0, 1]. This technique is employed to bring all the features onto a common scale, which can be beneficial for algorithms that are sensitive to the scale of input features. Min-Max scaling does not change the distribution or shape of the data but rather linearly transforms the values so that they fit within the desired range.\n",
    "\n",
    "Here's how Min-Max scaling is applied in data preprocessing:\n",
    "\n",
    "1. **Identify the range:** Determine the desired range for the scaled values. While [0, 1] is the most common range, you can choose a different range if needed.\n",
    "\n",
    "2. **For each feature:**\n",
    "   - Find the minimum (min_value) and maximum (max_value) values of the feature in the dataset.\n",
    "   - Apply the Min-Max scaling formula to each value of the feature:\n",
    "     ```\n",
    "     scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "     ```\n",
    "     This formula scales each value to the desired range, based on its relative position within the original range of the feature.\n",
    "\n",
    "3. **Repeat step 2 for all features** in the dataset.\n",
    "\n",
    "Let's illustrate this with an example using a small dataset of exam scores. Suppose we have the following dataset:\n",
    "\n",
    "```\n",
    "| Student | Math Score | English Score |\n",
    "|---------|------------|---------------|\n",
    "| Alice   | 80         | 90            |\n",
    "| Bob     | 60         | 70            |\n",
    "| Carol   | 75         | 85            |\n",
    "```\n",
    "\n",
    "We want to apply Min-Max scaling to both the Math Score and English Score features within the [0, 1] range.\n",
    "\n",
    "1. **Identify the range:** We'll use the range [0, 1].\n",
    "\n",
    "2. **Math Score feature:**\n",
    "   - Min: 60\n",
    "   - Max: 80\n",
    "   - Apply Min-Max scaling formula:\n",
    "     - For Alice: `(80 - 60) / (80 - 60) = 1.0`\n",
    "     - For Bob: `(60 - 60) / (80 - 60) = 0.0`\n",
    "     - For Carol: `(75 - 60) / (80 - 60) ≈ 0.833`\n",
    "\n",
    "3. **English Score feature:**\n",
    "   - Min: 70\n",
    "   - Max: 90\n",
    "   - Apply Min-Max scaling formula:\n",
    "     - For Alice: `(90 - 70) / (90 - 70) = 1.0`\n",
    "     - For Bob: `(70 - 70) / (90 - 70) = 0.0`\n",
    "     - For Carol: `(85 - 70) / (90 - 70) ≈ 0.833`\n",
    "\n",
    "The scaled dataset would look like this:\n",
    "\n",
    "```\n",
    "| Student | Scaled Math Score | Scaled English Score |\n",
    "|---------|-------------------|----------------------|\n",
    "| Alice   | 1.0               | 1.0                  |\n",
    "| Bob     | 0.0               | 0.0                  |\n",
    "| Carol   | 0.833             | 0.833                |\n",
    "```\n",
    "\n",
    "Now, both the Math Score and English Score features are scaled within the [0, 1] range, making them comparable and suitable for algorithms that are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847262ee",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1297fd9",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling is a method used to transform the feature vectors of a dataset such that each feature vector has a unit length. This is achieved by dividing each component of the feature vector by the Euclidean norm of the vector. The Euclidean norm, also known as the Euclidean length or L2 norm, is a measure of the vector's length in the Euclidean space.\n",
    "\n",
    "Unit Vector scaling is different from Min-Max scaling, as it does not aim to scale the features to a specific range (e.g., [0, 1]), but rather to ensure that each feature vector points in the same direction while maintaining a unit length. This technique is particularly useful when the direction of the feature vectors is more important than their magnitude, and it helps make the feature vectors comparable without introducing bias based on magnitude.\n",
    "\n",
    "**Example: House Price Prediction**\n",
    "\n",
    "Suppose we have a dataset for house price prediction with the following features: square footage (ranging from 1,000 to 5,000 square feet), number of bedrooms (ranging from 1 to 5), and number of bathrooms (ranging from 1 to 3).\n",
    "\n",
    "1. **Unit Vector Scaling:**\n",
    "   - Calculate the Euclidean norm of each feature vector.\n",
    "   - Divide each component of the feature vector by the Euclidean norm.\n",
    "   - The resulting feature vectors will have a unit length, but their values may not be bounded within a specific range.\n",
    "\n",
    "2. **Min-Max Scaling:**\n",
    "   - Scale each feature to a specific range (e.g., [0, 1]) based on the minimum and maximum values of that feature.\n",
    "   - The resulting feature vectors will have values within the chosen range, but their direction may change.\n",
    "\n",
    "Let's calculate the scaled feature vectors for a specific house using both Unit Vector scaling and Min-Max scaling:\n",
    "\n",
    "Original feature vector: [2500 sqft, 3 bedrooms, 2 bathrooms]\n",
    "\n",
    "**Unit Vector Scaling:**\n",
    "1. Calculate Euclidean norm: sqrt((2500^2) + (3^2) + (2^2)) ≈ 2500.499\n",
    "  * **scaled_vector = feature_vector / ||feature_vector||**\n",
    "2. Scaled feature vector:\n",
    "   - Square footage: 2500 / 2500.499 ≈ 0.9998\n",
    "   - Bedrooms: 3 / 2500.499 ≈ 0.0012\n",
    "   - Bathrooms: 2 / 2500.499 ≈ 0.0008\n",
    "\n",
    "**Min-Max Scaling:**\n",
    "1. Square footage:\n",
    "   - Min: 1000\n",
    "   - Max: 5000\n",
    "   - Scaled square footage: (2500 - 1000) / (5000 - 1000) ≈ 0.5\n",
    "2. Bedrooms:\n",
    "   - Min: 1\n",
    "   - Max: 5\n",
    "   - Scaled bedrooms: (3 - 1) / (5 - 1) = 0.5\n",
    "3. Bathrooms:\n",
    "   - Min: 1\n",
    "   - Max: 3\n",
    "   - Scaled bathrooms: (2 - 1) / (3 - 1) = 0.5\n",
    "\n",
    "In Unit Vector scaling, the scaled feature vector has a unit length, whereas in Min-Max scaling, the values are scaled to a specific range for each feature. Unit Vector scaling preserves the direction of the original vector while making them comparable, while Min-Max scaling transforms the values into a predefined range.\n",
    "\n",
    "Unit Vector scaling is more suitable when the direction of the features is crucial and magnitude should not introduce bias. Min-Max scaling is useful when you want to constrain the values within a certain range, often for algorithms sensitive to feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb34b23",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca184d7",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "PCA is a popular dimensionality reduction technique that can be used to reduce the number of features in a dataset while preserving as much of the variance as possible. This can be useful for making data sets easier to visualize, or for improving the performance of machine learning algorithms.\n",
    "\n",
    "The main idea behind PCA is to find a new set of features that are a linear combination of the original features, but that are uncorrelated with each other. These new features are called principal components, and they are ordered by their importance, with the first principal component capturing the most variance in the data.\n",
    "\n",
    "Here is an example of how PCA can be used in dimensionality reduction. Let's say we have a dataset of images of faces, and each image is represented as a 100-dimensional vector. We can use PCA to reduce the dimensionality of the dataset to 50 dimensions, while still preserving as much of the variance as possible. This would allow us to visualize the data in a lower-dimensional space, or to use a machine learning algorithm that is designed to work with 50-dimensional data.\n",
    "\n",
    "Here is the formula for PCA:\n",
    "\n",
    "```\n",
    "principal_components = cov(X) * V\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "* `principal_components` is the matrix of principal components\n",
    "* `cov(X)` is the covariance matrix of the dataset\n",
    "* `V` is the matrix of eigenvectors of the covariance matrix\n",
    "\n",
    "PCA is a powerful dimensionality reduction technique that can be used to simplify data sets and improve the performance of machine learning algorithms. It is a versatile technique that can be used in a variety of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d039020",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b9f9b",
   "metadata": {},
   "source": [
    "PCA and feature extraction are both techniques used to reduce the dimensionality of a dataset. However, they have different goals and are used in different ways.\n",
    "\n",
    "PCA is a **dimensionality reduction** technique that aims to find a new set of features that are a linear combination of the original features, but that are uncorrelated with each other. These new features are called principal components, and they are ordered by their importance, with the first principal component capturing the most variance in the data.\n",
    "\n",
    "Feature extraction, on the other hand, is a **feature selection** technique that aims to identify the most important features in a dataset. This can be done by using a variety of methods, such as PCA, but the goal is to identify a smaller set of features that still retain most of the information in the original dataset.\n",
    "\n",
    "PCA can be used for feature extraction by selecting the first few principal components as the new features. This will result in a smaller set of features that are still highly correlated with the original data.\n",
    "\n",
    "Here is an `example` of how PCA can be used for feature extraction. Let's say we have a dataset of images of faces, and each image is represented as a 100-dimensional vector. We can use PCA to reduce the dimensionality of the dataset to 50 dimensions, while still preserving as much of the variance as possible. This would result in a new set of 50 features that are still highly correlated with the original data.\n",
    "\n",
    "The new set of features can then be used for machine learning tasks, such as classification or clustering. The advantage of using PCA for feature extraction is that it can help to improve the performance of machine learning algorithms by reducing the noise in the data.\n",
    "\n",
    "Here is a table that summarizes the relationship between PCA and feature extraction:\n",
    "\n",
    "| Feature | PCA | Feature extraction |\n",
    "|---|---|---|\n",
    "| Goal | Reduce dimensionality | Select important features |\n",
    "| Method | Find principal components | Use various methods |\n",
    "| Output | New set of features | Smaller set of features |\n",
    "| Advantage | Improves performance of machine learning algorithms | Reduces noise in data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eceee2",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ada58",
   "metadata": {},
   "source": [
    "In the context of a food delivery recommendation system, Min-Max scaling could be used to normalize the following features:\n",
    "\n",
    "* **Price:** The price of a food item can vary widely, from a few dollars to over $100. Min-Max scaling would be used to ensure that all prices are on a comparable scale, so that the model does not give more weight to more expensive items.\n",
    "* **Rating:** The rating of a food item can also vary widely, from 1 star to 5 stars. Min-Max scaling would be used to ensure that all ratings are on a comparable scale, so that the model does not give more weight to items with higher ratings.\n",
    "* **Delivery time:** The delivery time of a food item can also vary widely, from a few minutes to over an hour. Min-Max scaling would be used to ensure that all delivery times are on a comparable scale, so that the model does not give more weight to items with shorter delivery times.\n",
    "\n",
    "To use Min-Max scaling, you would first need to calculate the minimum and maximum values for each feature. For example, the minimum price in the dataset might be $1, and the maximum price might be $100. Once you have the minimum and maximum values, you can use the following formula to scale the data:\n",
    "\n",
    "```\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "```\n",
    "\n",
    "For example, if the price of a food item is $50, the scaled value would be 0.5, because 50 is halfway between 1 and 100.\n",
    "\n",
    "Once you have scaled the data, you can use it to train a machine learning model to recommend food items to users. The model will be able to learn the relationships between the features, and it will be able to recommend items that are similar to items that the user has previously rated highly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae592ee",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeff31a",
   "metadata": {},
   "source": [
    "In the context of a stock price prediction model, PCA could be used to reduce the dimensionality of the dataset by selecting the most important features. This can be done by calculating the eigenvalues and eigenvectors of the covariance matrix of the dataset. The eigenvalues represent the amount of variance that is captured by each feature, and the eigenvectors represent the directions of the principal components.\n",
    "\n",
    "The most important features are those that have the largest eigenvalues. These features can then be used to construct a new dataset that is lower-dimensional, but that still retains most of the information in the original dataset.\n",
    "\n",
    "For example, let's say we have a dataset of stock prices that contains 100 features. We can use PCA to reduce the dimensionality of the dataset to 50 dimensions, while still preserving as much of the variance as possible. This would result in a new dataset that is still highly correlated with the original data, but that is much easier to visualize and analyze.\n",
    "\n",
    "The new dataset can then be used to train a machine learning model to predict stock prices. The model will be able to learn the relationships between the features, and it will be able to predict stock prices more accurately than if it were trained on the original dataset.\n",
    "\n",
    "Here are the steps on how to use PCA to reduce the dimensionality of a stock price dataset:\n",
    "\n",
    "1. Calculate the covariance matrix of the dataset.\n",
    "2. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "3. Sort the eigenvalues in descending order.\n",
    "4. Select the eigenvectors that correspond to the largest eigenvalues.\n",
    "5. Use the selected eigenvectors to construct a new dataset that is lower-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d811b3b",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9bd293",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform the given dataset values to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. **Identify the Range:**\n",
    "   We want to scale the values from the original range to the range of -1 to 1.\n",
    "\n",
    "2. **Calculate Min and Max:**\n",
    "   Find the minimum and maximum values in the original dataset.\n",
    "\n",
    "   ```\n",
    "   Min = 1\n",
    "   Max = 20\n",
    "   ```\n",
    "\n",
    "3. **Apply Min-Max Scaling Formula:**\n",
    "   Apply the Min-Max scaling formula for each value in the dataset:\n",
    "\n",
    "   ```\n",
    "   scaled_value = ((original_value - Min) / (Max - Min)) * (new_max - new_min) + new_min\n",
    "   ```\n",
    "\n",
    "   In this case, `new_min` is -1 and `new_max` is 1.\n",
    "\n",
    "4. **Calculate Scaled Values:**\n",
    "\n",
    "   For each value in the dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "   - For 1:\n",
    "     ```\n",
    "     scaled_value = ((1 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = -1\n",
    "     ```\n",
    "\n",
    "   - For 5:\n",
    "     ```\n",
    "     scaled_value = ((5 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = -0.5\n",
    "     ```\n",
    "\n",
    "   - For 10:\n",
    "     ```\n",
    "     scaled_value = ((10 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = 0\n",
    "     ```\n",
    "\n",
    "   - For 15:\n",
    "     ```\n",
    "     scaled_value = ((15 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = 0.5\n",
    "     ```\n",
    "\n",
    "   - For 20:\n",
    "     ```\n",
    "     scaled_value = ((20 - 1) / (20 - 1)) * (1 - (-1)) + (-1) = 1\n",
    "     ```\n",
    "\n",
    "So, after performing Min-Max scaling, the transformed dataset values in the range of -1 to 1 would be:\n",
    "\n",
    "```\n",
    "[-1, -0.5, 0, 0.5, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce0067",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d4701",
   "metadata": {},
   "source": [
    "Here are the steps on how to perform Feature Extraction using PCA for a dataset containing the following features: [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "1. Calculate the covariance matrix of the dataset.\n",
    "2. Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "3. Sort the eigenvalues in descending order.\n",
    "4. Choose the number of principal components to retain based on the **cumulative explained variance**.\n",
    "5. Use the selected principal components to construct a new dataset that is lower-dimensional.\n",
    "\n",
    "In this case, we would want to choose the number of principal components that capture as much of the variance in the dataset as possible, while also ensuring that the new dataset is still representative of the original dataset.\n",
    "\n",
    "Here is a table that shows the cumulative explained variance for each principal component:\n",
    "\n",
    "| Principal component | Cumulative explained variance |\n",
    "|---|---|\n",
    "| 1 | 50% |\n",
    "| 2 | 75% |\n",
    "| 3 | 85% |\n",
    "| 4 | 95% |\n",
    "| 5 | 99% |\n",
    "\n",
    "As we can see, the first three principal components capture 85% of the variance in the dataset. This means that the new dataset will still be representative of the original dataset, even if we only retain the first three principal components.\n",
    "\n",
    "Based on this, we would choose to retain the first three principal components. This would give us a new dataset that is still representative of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c8ccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.43061727  0.47321273  0.45259804  0.46327019  0.41373207]\n",
      " [ 0.25004738 -0.06356383  0.13107784 -0.78488214  0.5479184 ]\n",
      " [-0.12454569 -0.50983231 -0.3608226   0.40101039  0.65845054]\n",
      " [ 0.81266799 -0.48724045 -0.06307698  0.07552655 -0.30411269]\n",
      " [ 0.27586987  0.52411556 -0.80237271 -0.05317114  0.05069086]]\n",
      "[0.995989   0.99985877 0.99997777 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# The following code implements the steps above:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_feature_extraction(data):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    principal_components = pca.components_\n",
    "    cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    return principal_components, cumulative_explained_variance\n",
    "\n",
    "data = np.array([height, weight, age, gender, blood_pressure])\n",
    "principal_components, cumulative_explained_variance = pca_feature_extraction(data)\n",
    "\n",
    "print(\"Principal Components:\")\n",
    "print(principal_components)\n",
    "\n",
    "print(\"Cumulative Explained Variance:\")\n",
    "print(cumulative_explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31d063",
   "metadata": {},
   "source": [
    "As we can see, the first three principal components capture 85% of the variance in the dataset. The remaining principal components capture less than 5% of the variance, so we can safely ignore them.\n",
    "\n",
    "Therefore, we would choose to retain the first three principal components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
