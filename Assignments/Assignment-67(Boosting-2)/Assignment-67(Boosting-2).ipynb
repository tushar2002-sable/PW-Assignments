{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d3a9ce",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e4d5b",
   "metadata": {},
   "source": [
    "Gradient boosting regression is a machine learning technique that uses a series of weak learners to fit a regression model. The weak learners are typically decision trees, and they are trained sequentially, with each learner being trained to correct the mistakes of the previous learner.\n",
    "\n",
    "The gradient boosting regression algorithm works by iteratively fitting a sequence of weak learners to the residual errors of the previous learners. The residual errors are the differences between the actual target values and the predictions of the previous learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a2eb9",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d662d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25e427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset for regression\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b03543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of boosting iterations\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb0e464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the predictions with the mean of the target variable\n",
    "predictions = np.full_like(y, np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6b8a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "for i in range(n_estimators):\n",
    "    # Compute the negative gradient (residuals) with respect to the current predictions\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Fit a weak learner (a decision tree stump in this case) to the residuals\n",
    "    # You can replace this with a more complex model if needed\n",
    "    stump = DecisionTreeRegressor(max_depth=1)\n",
    "    stump.fit(X, residuals)\n",
    "    \n",
    "    # Make predictions with the weak learner\n",
    "    weak_predictions = stump.predict(X)\n",
    "    \n",
    "    # Update the predictions with a fraction (learning_rate) of the weak learner's predictions\n",
    "    predictions += learning_rate * weak_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363cfaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.86\n",
      "R-squared (R2): 0.34\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19259b4d",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3f529",
   "metadata": {},
   "source": [
    "Here are the steps on how to experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the gradient boosting model using grid search or random search:\n",
    "\n",
    "1. Define the hyperparameters that you want to experiment with. In this case, the hyperparameters are the learning rate, the number of trees, and the tree depth.\n",
    "2. Choose a hyperparameter search method. Grid search is a brute-force method that tries all possible combinations of the hyperparameters. Random search is a more efficient method that randomly explores the hyperparameter space.\n",
    "3. Set up the hyperparameter search. This involves specifying the values of the hyperparameters that you want to explore, as well as the number of trials that you want to run.\n",
    "4. Run the hyperparameter search. This will train the gradient boosting model with different hyperparameter settings and evaluate its performance.\n",
    "5. Select the best hyperparameters. The hyperparameters that result in the best performance are the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c50861db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the hyperparameters.\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
    "number_of_trees = [10, 20, 50, 100]\n",
    "tree_depths = [1, 2, 3, 4]\n",
    "\n",
    "# Set up the grid search.\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(),\n",
    "    param_grid={\n",
    "        \"learning_rate\": learning_rates,\n",
    "        \"n_estimators\": number_of_trees,\n",
    "        \"max_depth\": tree_depths,\n",
    "    },\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "# Run the grid search.\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Select the best hyperparameters.\n",
    "best_hyperparameters = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6917afc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 10}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f401056",
   "metadata": {},
   "source": [
    "In this example, we are using 5-fold cross-validation to evaluate the performance of the model. This means that the model will be trained and evaluated 5 times, each time with a different 5-fold split of the data.\n",
    "\n",
    "The best hyperparameters are the ones that result in the lowest mean squared error (MSE) on the cross-validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59e540",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f2013",
   "metadata": {},
   "source": [
    "In gradient boosting, a weak learner is a machine learning model that is only slightly better than random guessing. Weak learners are typically decision trees with a small number of leaves.\n",
    "\n",
    "The gradient boosting algorithm works by iteratively adding weak learners to a model. Each weak learner is trained to correct the mistakes of the previous weak learners. This process is repeated until the desired accuracy is achieved.\n",
    "\n",
    "The use of weak learners makes gradient boosting a very powerful machine learning technique. This is because it allows the algorithm to learn from its mistakes and improve its performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5346ff",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4985b",
   "metadata": {},
   "source": [
    "The intuition behind the gradient boosting algorithm is to build a model that minimizes the **residual errors** of the previous model. The residual errors are the differences between the predicted values and the actual values.\n",
    "\n",
    "The gradient boosting algorithm works by iteratively adding weak learners to a model. Each weak learner is trained to minimize the residual errors of the previous model. This process is repeated until the desired accuracy is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd24a2a",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6fd71",
   "metadata": {},
   "source": [
    "Gradient boosting builds an ensemble of weak learners by iteratively adding weak learners to a model. Each weak learner is trained to minimize the **residual errors** of the previous model. The residual errors are the differences between the predicted values and the actual values.\n",
    "\n",
    "The gradient boosting algorithm works as follows:\n",
    "\n",
    "1. The gradient boosting algorithm starts with an initial model, such as a simple decision tree.\n",
    "2. The algorithm then calculates the residual errors of the initial model.\n",
    "3. The algorithm then trains a new weak learner to minimize the residual errors of the initial model.\n",
    "4. The predictions of the new weak learner are added to the predictions of the initial model.\n",
    "5. The algorithm repeats steps 2-4 until the desired accuracy is achieved.\n",
    "\n",
    "The weak learners are typically decision trees with a small number of leaves. The algorithm adds the weak learners sequentially, with each weak learner being trained to correct the mistakes of the previous weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f41f5",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5372a",
   "metadata": {},
   "source": [
    "The mathematical intuition of gradient boosting can be constructed in the following steps:\n",
    "\n",
    "1. **Define the loss function:** The loss function is a measure of how well the model fits the data. The most common loss function for gradient boosting is the **squared error loss function**.\n",
    "2. **Initialize the model:** The model is initialized to a simple model, such as a constant or a linear model.\n",
    "3. **Calculate the residual errors:** The residual errors are the differences between the predicted values and the actual values.\n",
    "4. **Train a weak learner to minimize the residual errors:** A weak learner is trained to minimize the residual errors. The weak learner can be a decision tree, a linear model, or another type of model.\n",
    "5. **Update the predictions:** The predictions of the model are updated by adding the predictions of the weak learner.\n",
    "6. **Repeat steps 3-5 until the desired accuracy is achieved:** The steps are repeated until the desired accuracy is achieved.\n",
    "\n",
    "The gradient boosting algorithm can be thought of as a way of **iteratively reducing the loss function**. The loss function is reduced by adding weak learners that are specifically designed to minimize the residual errors.\n",
    "\n",
    "The mathematical intuition of gradient boosting can be used to understand how the algorithm works and to choose the hyperparameters of the algorithm.\n",
    "\n",
    "Here are some of the hyperparameters of gradient boosting:\n",
    "\n",
    "* **Number of trees:** The number of weak learners that are added to the model.\n",
    "* **Learning rate:** The weight that is given to the predictions of the weak learners.\n",
    "* **Tree depth:** The depth of the weak learners.\n",
    "\n",
    "The hyperparameters of gradient boosting can be tuned to improve the performance of the model.\n",
    "\n",
    "Overall, the mathematical intuition of gradient boosting is a powerful tool that can be used to understand and improve the performance of the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
