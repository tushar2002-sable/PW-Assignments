{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f496fc",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a07c7",
   "metadata": {},
   "source": [
    "Let's say there are 100 employees in the company. 70% of them use the health insurance plan, so there are 70 employees who use the plan. 40% of the employees who use the plan are smokers, so there are 0.4 * 70 = 28 employees who are smokers and use the plan.\n",
    "\n",
    "The probability that an employee is a smoker given that he/she uses the health insurance plan is:\n",
    "\n",
    "```\n",
    "P(smoker|uses plan) = 28/70 = 7/17\n",
    "```\n",
    "\n",
    "This means that an employee who uses the health insurance plan is 7 times more likely to be a smoker than an employee who does not use the plan.\n",
    "\n",
    "Here is the solution in code:\n",
    "\n",
    "```python\n",
    "def smoker_probability(uses_plan):\n",
    "  return 28 / 70\n",
    "\n",
    "print(smoker_probability(True))  # 7/17\n",
    "print(smoker_probability(False))  # 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82fe6b",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2251bd0",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both supervised machine learning algorithms that are used for classification tasks. They are both based on Bayes' theorem, but they make different assumptions about the data.\n",
    "\n",
    "**Bernoulli Naive Bayes** assumes that the features are binary, meaning that they can take on only two values, such as \"present\" or \"absent\". For example, a Bernoulli Naive Bayes classifier could be used to classify whether a document is spam or not, based on the presence or absence of certain words.\n",
    "\n",
    "**Multinomial Naive Bayes** assumes that the features are categorical, meaning that they can take on a limited number of values. For example, a Multinomial Naive Bayes classifier could be used to classify the genre of a movie, based on the words that are used in the movie's title.\n",
    "\n",
    "In both Bernoulli Naive Bayes and Multinomial Naive Bayes, the features are assumed to be independent of each other. This means that the probability of a particular feature value occurring is not affected by the value of any other feature.\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is how they handle the features. Bernoulli Naive Bayes treats each feature as a binary variable, while Multinomial Naive Bayes treats each feature as a categorical variable.\n",
    "\n",
    "Bernoulli Naive Bayes is a simpler model than Multinomial Naive Bayes, and it is often faster to train. However, Multinomial Naive Bayes is more flexible and can be used to model more complex data sets.\n",
    "\n",
    "Here is a table that summarizes the key differences between Bernoulli Naive Bayes and Multinomial Naive Bayes:\n",
    "\n",
    "| Feature | Bernoulli Naive Bayes | Multinomial Naive Bayes |\n",
    "|---|---|---|\n",
    "| Type of features | Binary | Categorical |\n",
    "| Speed | Faster | Slower |\n",
    "| Flexibility | Less flexible | More flexible |\n",
    "| Datasets | Works well with simple data sets | Works well with complex data sets |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f3014",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c328da",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by assuming that the missing values are **equally likely to be either present or absent**. This means that the probability of a missing value being present is the same as the probability of a missing value being absent.\n",
    "\n",
    "For example, let's say we have a dataset of documents, and one of the features is the presence of the word \"the\". If a document is missing the value for this feature, Bernoulli Naive Bayes will assume that the document is equally likely to have the word \"the\" or not have the word \"the\".\n",
    "\n",
    "This assumption can lead to overfitting, which is when the model learns the noise in the data instead of the underlying patterns. To avoid overfitting, it is important to use a regularization technique, such as **Lasso** or **Ridge regression**.\n",
    "\n",
    "Here are some other ways to handle missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "* **Impute the missing values:** This means replacing the missing values with some other value, such as the mean or median of the values for that feature.\n",
    "* **Drop the instances with missing values:** This means removing the instances from the dataset that have missing values.\n",
    "* **Use a different classification algorithm:** There are other classification algorithms that are more robust to missing values, such as **Decision Trees** or **Random Forests**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110892b",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5566441",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a supervised machine learning algorithm that is used for classification tasks. It is based on Bayes' theorem, and it assumes that the features are normally distributed.\n",
    "\n",
    "In multi-class classification, there are multiple classes that the data can be classified into. For example, a multi-class classification problem could be to classify images of flowers into different types of flowers.\n",
    "\n",
    "Gaussian Naive Bayes can be used for multi-class classification by assuming that the features are normally distributed within each class. This means that the probability of a feature value occurring is given by a normal distribution.\n",
    "\n",
    "Gaussian Naive Bayes is a simple and efficient algorithm that can be used for multi-class classification. However, it is important to note that it makes the assumption that the features are normally distributed. This assumption may not be valid for all data sets, and it can lead to poor performance if the assumption is not met.\n",
    "\n",
    "Here are some other algorithms that can be used for multi-class classification:\n",
    "\n",
    "* **Support Vector Machines:** Support Vector Machines (SVMs) are a powerful machine learning algorithm that can be used for both binary and multi-class classification. SVMs work by finding a hyperplane that separates the different classes in the data.\n",
    "* **Decision Trees:** Decision Trees are a simple and intuitive machine learning algorithm that can be used for both binary and multi-class classification. Decision Trees work by splitting the data into smaller and smaller groups until each group belongs to a single class.\n",
    "* **Random Forests:** Random Forests are an ensemble learning algorithm that combines multiple decision trees to improve the performance of the model. Random Forests work by randomly selecting features and splitting the data into smaller and smaller groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e85f5",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "**Data preparation:**\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default \n",
    "hyperparameters for each classifier.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "**Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b221a50",
   "metadata": {},
   "source": [
    "\n",
    "Here are the steps involved in implementing Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python:\n",
    "\n",
    "* Import the necessary libraries.\n",
    "* Load the Spambase dataset.\n",
    "* Split the dataset into a training set and a test set.\n",
    "* Create three classifiers: a Bernoulli Naive Bayes classifier, a Multinomial Naive Bayes classifier, and a Gaussian Naive Bayes classifier.\n",
    "* Train each classifier on the training set.\n",
    "* Evaluate each classifier on the test set.\n",
    "* Report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1116def2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8863499699338545\n",
      "Recall: 0.8130170987313844\n",
      "F1 score: 0.8481012658227849\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7488399071925754\n",
      "Recall: 0.7120794263651407\n",
      "F1 score: 0.7299971727452643\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7009307972480777\n",
      "Recall: 0.9553226696083839\n",
      "F1 score: 0.8085901027077498\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "data = pd.read_csv('spambase.csv')\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Labels\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create three classifiers instances\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train each classifier on the training set\n",
    "bnb.fit(X_train, y_train)\n",
    "mnb.fit(X_train, y_train)\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "bnb_scores = cross_val_score(bnb, X, y, cv=10, scoring='accuracy')\n",
    "mnb_scores = cross_val_score(mnb, X, y, cv=10, scoring='accuracy')\n",
    "gnb_scores = cross_val_score(gnb, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Calculate the performance metrics from cross-validation scores\n",
    "bnb_accuracy = np.mean(bnb_scores)\n",
    "bnb_precision = precision_score(y, bnb.predict(X))\n",
    "bnb_recall = recall_score(y, bnb.predict(X))\n",
    "bnb_f1 = f1_score(y, bnb.predict(X))\n",
    "\n",
    "mnb_accuracy = np.mean(mnb_scores)\n",
    "mnb_precision = precision_score(y, mnb.predict(X))\n",
    "mnb_recall = recall_score(y, mnb.predict(X))\n",
    "mnb_f1 = f1_score(y, mnb.predict(X))\n",
    "\n",
    "gnb_accuracy = np.mean(gnb_scores)\n",
    "gnb_precision = precision_score(y, gnb.predict(X))\n",
    "gnb_recall = recall_score(y, gnb.predict(X))\n",
    "gnb_f1 = f1_score(y, gnb.predict(X))\n",
    "\n",
    "# Print the results\n",
    "print('Bernoulli Naive Bayes:')\n",
    "print('Accuracy:', bnb_accuracy)\n",
    "print('Precision:', bnb_precision)\n",
    "print('Recall:', bnb_recall)\n",
    "print('F1 score:', bnb_f1)\n",
    "\n",
    "print('Multinomial Naive Bayes:')\n",
    "print('Accuracy:', mnb_accuracy)\n",
    "print('Precision:', mnb_precision)\n",
    "print('Recall:', mnb_recall)\n",
    "print('F1 score:', mnb_f1)\n",
    "\n",
    "print('Gaussian Naive Bayes:')\n",
    "print('Accuracy:', gnb_accuracy)\n",
    "print('Precision:', gnb_precision)\n",
    "print('Recall:', gnb_recall)\n",
    "print('F1 score:', gnb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe5a1b",
   "metadata": {},
   "source": [
    "**Performance Summary:**\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - Accuracy: 88.4%\n",
    "   - Precision: 88.6%\n",
    "   - Recall: 81.3%\n",
    "   - F1 Score: 84.8%\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - Accuracy: 78.6%\n",
    "   - Precision: 74.9%\n",
    "   - Recall: 71.2%\n",
    "   - F1 Score: 73.0%\n",
    "\n",
    "3. **Gaussian Naive Bayes:**\n",
    "   - Accuracy: 82.2%\n",
    "   - Precision: 70.1%\n",
    "   - Recall: 95.5%\n",
    "   - F1 Score: 80.9%\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "1. **Bernoulli Naive Bayes**: \n",
    "   - **Advantages**: Bernoulli Naive Bayes performed the best in terms of accuracy, precision, and F1 score. It's particularly well-suited for binary classification problems like spam detection because it models features as binary values (presence or absence).\n",
    "   - **Why It Performed Well**: It's likely that the dataset's binary nature (presence or absence of words or features) aligns well with the assumptions of the Bernoulli Naive Bayes model.\n",
    "   - **Limitations**: While it performed well, Bernoulli Naive Bayes may struggle with features that are not naturally binary or when there is a need to capture the frequency or intensity of certain terms, which it doesn't do effectively.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - **Advantages**: Multinomial Naive Bayes is commonly used for text classification, but in this case, it didn't perform as well as Bernoulli Naive Bayes. It's suitable when features represent counts or frequencies.\n",
    "   - **Limitations**: It might not perform optimally when the data is not well-modeled as multinomially distributed counts, which could be the case with binary presence/absence features.\n",
    "\n",
    "3. **Gaussian Naive Bayes**:\n",
    "   - **Advantages**: Gaussian Naive Bayes is designed for continuous data. It achieved a high recall rate, meaning it identified a significant portion of actual spam emails.\n",
    "   - **Limitations**: Gaussian Naive Bayes assumes that features follow a Gaussian distribution, which might not be appropriate for text data, where features are typically not normally distributed. This assumption might not hold well in practice.\n",
    "\n",
    "**Findings and Suggestions:**\n",
    "\n",
    "- **Best Performer**: Bernoulli Naive Bayes was the best performer overall, striking a good balance between precision and recall, which is crucial for spam detection where false positives (non-spam emails classified as spam) and false negatives (spam emails classified as non-spam) both have implications.\n",
    "\n",
    "- **Limitations of Naive Bayes**: Naive Bayes models make strong independence assumptions between features, which might not always hold in real-world data. Additionally, the choice of variant (Bernoulli, Multinomial, or Gaussian) depends on the nature of the data and its distribution.\n",
    "\n",
    "- **Future Work**: To further improve spam classification, consider the following:\n",
    "  - Feature Engineering: Experiment with different text preprocessing techniques, such as TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings like Word2Vec.\n",
    "  - Hyperparameter Tuning: Optimize hyperparameters for each Naive Bayes variant or explore other classification algorithms like Random Forests, Support Vector Machines, or deep learning methods.\n",
    "  - Ensembling: Combine predictions from multiple classifiers to improve overall performance.\n",
    "  - Anomaly Detection: Consider using anomaly detection methods to identify unusual patterns in spam emails.\n",
    "  - Continuous Data Handling: If you want to leverage continuous data (e.g., email metadata), explore models better suited for such data.\n",
    "\n",
    "In conclusion, Bernoulli Naive Bayes performed the best on the given dataset, but the choice of algorithm should be tailored to the specific characteristics of the data and the goals of the spam detection task. Further optimization and experimentation can lead to even better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
