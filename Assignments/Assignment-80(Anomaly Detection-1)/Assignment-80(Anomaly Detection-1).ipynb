{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6de793",
   "metadata": {},
   "source": [
    "### Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ccd564",
   "metadata": {},
   "source": [
    "Anomaly detection is the process of identifying data points, events, or items that deviate significantly from the normal behavior or patterns. Anomalies are often indicative of a problem, such as a fault or an intrusion, and may require further investigation or action.\n",
    "\n",
    "The purpose of anomaly detection is to identify anomalies so that they can be investigated and resolved. This can help to prevent problems, such as fraud, security breaches, and equipment failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e63d5",
   "metadata": {},
   "source": [
    "### Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ed881",
   "metadata": {},
   "source": [
    "Anomaly detection is the process of identifying data points that deviate significantly from the normal behavior or patterns. It is a challenging problem, as it is difficult to define what is normal and what is anomalous.\n",
    "\n",
    "Here are some of the key challenges in anomaly detection:\n",
    "\n",
    "* **Data quality:** Anomaly detection algorithms rely on high-quality data. If the data is noisy or incomplete, it can be difficult to identify anomalies.\n",
    "* **Concept drift:** The normal behavior or patterns in data can change over time. This is known as concept drift. Anomaly detection algorithms need to be able to adapt to concept drift in order to be effective.\n",
    "* **Rarity of anomalies:** Anomalies are, by definition, rare. This can make it difficult to train anomaly detection algorithms, as they may not see enough examples of anomalies during training.\n",
    "* **High dimensionality of data:** Real-world data is often high-dimensional, meaning that it has many features. This can make it difficult to develop anomaly detection algorithms that can effectively identify anomalies in high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67fb79",
   "metadata": {},
   "source": [
    "### Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0dde36",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two different approaches to anomaly detection.\n",
    "\n",
    "**Unsupervised anomaly detection** does not require any labeled data. Instead, it uses unsupervised learning algorithms to learn the normal behavior or patterns in the data and then identify anomalies as data points that deviate significantly from the normal behavior or patterns.\n",
    "\n",
    "**Supervised anomaly detection** requires labeled data. This data consists of data points that have been labeled as either normal or anomalous. The supervised anomaly detection algorithm learns from this labeled data and then uses this knowledge to identify anomalies in new data.\n",
    "\n",
    "**Here is a table that summarizes the key differences between unsupervised and supervised anomaly detection:**\n",
    "\n",
    "| Characteristic | Unsupervised anomaly detection | Supervised anomaly detection |\n",
    "|---|---|---|\n",
    "| Labeled data | Not required | Required |\n",
    "| Learning algorithm | Unsupervised learning | Supervised learning |\n",
    "| Strengths | Can detect any type of anomaly, including anomalies that are not known in advance | More accurate than unsupervised anomaly detection |\n",
    "| Weaknesses | Can be less accurate than supervised anomaly detection | Requires labeled data, which can be expensive and time-consuming to collect |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324872e",
   "metadata": {},
   "source": [
    "### Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301972f",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be broadly classified into three main categories:\n",
    "\n",
    "* **Statistical methods:** Statistical methods use statistical models to learn the normal behavior or patterns in the data and then identify anomalies as data points that deviate significantly from the normal behavior or patterns. Examples of statistical methods for anomaly detection include:\n",
    "    * Z-score test\n",
    "    * Grubbs's test\n",
    "    * Dixon's test\n",
    "    * Gaussian Mixture Models (GMMs)\n",
    "    * One-Class Support Vector Machines (OC-SVMs)\n",
    "* **Distance-based methods:** Distance-based methods measure the distance between each data point and the other data points in the dataset. Anomalies are identified as data points that are far away from most of the other data points. Examples of distance-based methods for anomaly detection include:\n",
    "    * K-Nearest Neighbors (KNN)\n",
    "    * Local Outlier Factor (LOF)\n",
    "    * Isolation Forests\n",
    "* **Clustering-based methods:** Clustering-based methods group similar data points together into clusters. Anomalies are identified as data points that do not belong to any of the clusters. Examples of clustering-based methods for anomaly detection include:\n",
    "    * Hierarchical clustering\n",
    "    * K-Means clustering\n",
    "    * DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dbcf86",
   "metadata": {},
   "source": [
    "### Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a65c4f",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods are based on the assumption that anomalous data points are far away from most of the other data points in the dataset. This is known as the **distance assumption**.\n",
    "\n",
    "In addition to the distance assumption, distance-based anomaly detection methods also make the following assumptions:\n",
    "\n",
    "* The data is distributed in a meaningful way. This means that there are natural clusters in the data.\n",
    "* The anomalous data points are rare. This means that the number of anomalous data points is much smaller than the number of normal data points.\n",
    "* The distance between data points is a good measure of similarity. This means that data points that are close together are more similar than data points that are far apart.\n",
    "\n",
    "If these assumptions are not met, then distance-based anomaly detection methods may not be effective. For example, if the data is not distributed in a meaningful way, then it will be difficult to identify anomalous data points. Similarly, if the anomalous data points are not rare, then distance-based anomaly detection methods may flag too many data points as anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795fdcc",
   "metadata": {},
   "source": [
    "### Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9eae6b",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores for data points based on their local density. The LOF score of a data point is calculated by comparing the density of the data point to the density of its neighbors. Data points with a lower density than their neighbors are considered to be more anomalous.\n",
    "\n",
    "To calculate the LOF score of a data point, the LOF algorithm first finds the k-nearest neighbors of the data point. The k-nearest neighbors are the k data points that are closest to the data point in terms of distance.\n",
    "\n",
    "Once the k-nearest neighbors of the data point have been found, the LOF algorithm calculates the local reachability density of the data point. The local reachability density of a data point is the average distance from the data point to its k-nearest neighbors.\n",
    "\n",
    "Data points with a LOF score greater than 1 are considered to be anomalous. The higher the LOF score of a data point, the more anomalous it is.\n",
    "\n",
    "The LOF algorithm is a popular anomaly detection algorithm because it is effective in detecting anomalies in a wide variety of data types. It is also relatively simple to understand and implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85827b8b",
   "metadata": {},
   "source": [
    "### Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bafa6",
   "metadata": {},
   "source": [
    "The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "* **n_estimators:** The number of trees to build in the forest. A higher number of trees will generally result in a more accurate model, but it will also take longer to train.\n",
    "* **max_samples:** The number of samples to draw from the training data when building each tree. A higher value of max_samples will generally produce more accurate trees, but it will also make the trees more susceptible to overfitting.\n",
    "* **contamination:** The proportion of outliers in the training data. This parameter is used to calculate the anomaly score of each data point. A higher value of contamination will result in higher anomaly scores for outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ec81f",
   "metadata": {},
   "source": [
    "### Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de2932",
   "metadata": {},
   "source": [
    "If a data point has only 2 neighbors of the same class within a radius of 0.5, its anomaly score using KNN with K=10 will be **high**. This is because the data point is very different from its neighbors, which suggests that it is an outlier.\n",
    "\n",
    "To calculate the anomaly score using KNN, we first need to find the k-nearest neighbors of the data point. The k-nearest neighbors are the k data points that are closest to the data point in terms of distance.\n",
    "\n",
    "Once we have found the k-nearest neighbors of the data point, we can calculate the anomaly score of the data point as follows:\n",
    "\n",
    "```\n",
    "Anomaly score = (number of neighbors of different class) / (total number of neighbors)\n",
    "```\n",
    "\n",
    "In this case, the data point only has 2 neighbors of the same class within a radius of 0.5, which means that it has 8 neighbors of different class. Therefore, the anomaly score of the data point will be:\n",
    "\n",
    "```\n",
    "Anomaly score = 8 / 10 = 0.8\n",
    "```\n",
    "\n",
    "An anomaly score of 0.8 is considered to be high, which suggests that the data point is an outlier.\n",
    "\n",
    "It is important to note that the anomaly score is just a measure of how different a data point is from its neighbors. It does not necessarily mean that the data point is an outlier. For example, a data point that is simply on the edge of a cluster may have a high anomaly score. However, a data point with a high anomaly score is more likely to be an outlier than a data point with a low anomaly score.\n",
    "\n",
    "To determine whether a data point is actually an outlier, we need to consider other factors, such as the domain knowledge of the data and the specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80990e5",
   "metadata": {},
   "source": [
    "### Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc150d",
   "metadata": {},
   "source": [
    "The anomaly score of a data point in an Isolation Forest algorithm is calculated as the average path length of the data point in the forest. The average path length is the number of splits required to isolate the data point.\n",
    "\n",
    "In this case, the data point has an average path length of 5.0, which is higher than the average path length of the trees. This suggests that the data point is more difficult to isolate than the other data points in the dataset, and therefore it is more likely to be an outlier.\n",
    "\n",
    "To calculate the anomaly score of the data point, we can use the following formula:\n",
    "\n",
    "\n",
    "Anomaly score = (average path length of all data points in the forest) / (average path length of the data point)\n",
    "\n",
    "\n",
    "Assuming that the average path length of all data points in the forest is 4.0, the anomaly score of the data point would be:\n",
    "\n",
    "```\n",
    "Anomaly score = 4.0 / 5.0 = 0.80\n",
    "```\n",
    "\n",
    "An anomaly score of 0.80 is considered to be high, which suggests that the data point is an outlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
