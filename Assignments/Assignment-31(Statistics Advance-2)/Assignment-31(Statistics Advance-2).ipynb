{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3661a0a9",
   "metadata": {},
   "source": [
    "##### Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c63578",
   "metadata": {},
   "source": [
    "Probability Mass Function (PMF) and Probability Density Function (PDF) are both mathematical concepts used in probability theory and statistics to describe the probabilities of random variables.\n",
    "\n",
    "1. Probability Mass Function (PMF):\n",
    "The Probability Mass Function (PMF) is used for discrete random variables. It gives the probability of each possible outcome in a discrete probability distribution. In simpler terms, it tells us the likelihood of a particular value occurring in a discrete set of values.\n",
    "\n",
    "The PMF is denoted as P(X = x), where X is the random variable, and x is a specific value of the random variable X.\n",
    "\n",
    "Example:\n",
    "Consider rolling a fair six-sided die. The random variable X represents the outcome of the roll. The possible outcomes are {1, 2, 3, 4, 5, 6}. Since each outcome has an equal chance of occurring, the PMF for this scenario is:\n",
    "```\n",
    "P(X = 1) = 1/6\n",
    "P(X = 2) = 1/6\n",
    "P(X = 3) = 1/6\n",
    "P(X = 4) = 1/6\n",
    "P(X = 5) = 1/6\n",
    "P(X = 6) = 1/6\n",
    "```\n",
    "The sum of all these probabilities must equal 1, as one of these outcomes must occur when rolling the die.\n",
    "\n",
    "2. Probability Density Function (PDF):\n",
    "The Probability Density Function (PDF) is used for continuous random variables. It describes the likelihood of the random variable falling within a particular range of values. Unlike the PMF, the PDF doesn't provide the probability of a specific value but instead gives the probability density at each point in the range. The probability of a random variable falling within a range is the integral of the PDF over that range.\n",
    "\n",
    "The PDF is denoted as f(x) for the random variable X.\n",
    "\n",
    "Example:\n",
    "Let's consider a continuous random variable X that represents the height of people in a given population. The PDF for this scenario might look like a bell-shaped curve, such as the normal distribution.\n",
    "\n",
    "In this case, the PDF f(x) represents the probability density of finding a person with height x. The actual probability of someone having an exact height, say 175 cm, is infinitesimally small because the height can take any value within a range. The probability of finding someone within a certain height range, for example, between 170 cm and 180 cm, is represented by the integral of the PDF over that range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ad75b",
   "metadata": {},
   "source": [
    "##### Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f747966",
   "metadata": {},
   "source": [
    "The Cumulative Density Function (CDF) is a concept used in probability theory and statistics to describe the probability that a random variable X will take on a value less than or equal to a given value x. In other words, the CDF gives us the cumulative probability up to a certain point.\n",
    "\n",
    "Mathematically, for a random variable X, the CDF is defined as F(x) = P(X ≤ x).\n",
    "\n",
    "The CDF is applicable to both discrete and continuous random variables.\n",
    "\n",
    "Example:\n",
    "Let's consider the example of rolling a fair six-sided die, where the random variable X represents the outcome of the roll. The PMF for this scenario is already discussed in the previous question. Now, we'll look at the CDF.\n",
    "\n",
    "CDF of rolling a fair six-sided die:\n",
    "To find the CDF for this scenario, we calculate the cumulative probabilities for each outcome:\n",
    "```\n",
    "F(X ≤ 1) = P(X ≤ 1) = P(X = 1) = 1/6\n",
    "F(X ≤ 2) = P(X ≤ 2) = P(X = 1) + P(X = 2) = 1/6 + 1/6 = 2/6 = 1/3\n",
    "F(X ≤ 3) = P(X ≤ 3) = P(X = 1) + P(X = 2) + P(X = 3) = 1/6 + 1/6 + 1/6 = 3/6 = 1/2\n",
    "F(X ≤ 4) = P(X ≤ 4) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) = 1/6 + 1/6 + 1/6 + 1/6 = 4/6 = 2/3\n",
    "F(X ≤ 5) = P(X ≤ 5) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) + P(X = 5) = 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 5/6\n",
    "F(X ≤ 6) = P(X ≤ 6) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) + P(X = 5) + P(X = 6) = 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 6/6 = 1\n",
    "```\n",
    "The CDF gives us the cumulative probabilities of getting a value less than or equal to each outcome when rolling the die.\n",
    "\n",
    "Why CDF is used?\n",
    "The Cumulative Density Function (CDF) is used for several reasons:\n",
    "\n",
    "1. Calculation of probabilities: The CDF provides a simple way to calculate the probability of a random variable being less than or equal to a specific value. It gives a complete overview of the probabilities of different outcomes in one cumulative function.\n",
    "\n",
    "2. Understanding the distribution: The shape and behavior of the CDF can give insights into the distribution of the random variable. For example, for a continuous random variable, the CDF often reveals patterns such as skewness or symmetry.\n",
    "\n",
    "3. Percentile calculations: The CDF allows us to find percentiles easily. For example, to find the 75th percentile of a distribution, we look for the value of x for which F(x) = 0.75.\n",
    "\n",
    "4. Hypothesis testing and confidence intervals: The CDF is essential in hypothesis testing and constructing confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82c95d",
   "metadata": {},
   "source": [
    "##### Q3: What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553974b5",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution, is one of the most commonly used probability distributions in statistics. It is used as a model for a wide range of situations in various fields due to its properties and prevalence in natural phenomena. Some examples of situations where the normal distribution might be used as a model include:\n",
    "\n",
    "1. Heights and Weights: The heights and weights of a large population tend to follow a normal distribution. While there might be some variations, the overall pattern is often bell-shaped, making the normal distribution an appropriate model.\n",
    "\n",
    "2. IQ Scores: Intelligence quotient (IQ) scores are often modeled using the normal distribution, where the mean represents the average IQ in the population, and the standard deviation represents the spread of IQ scores around the mean.\n",
    "\n",
    "3. Errors in Measurements: In many scientific experiments and measurements, errors are expected. These errors are often assumed to be normally distributed, which allows researchers to make inferences and apply statistical tests.\n",
    "\n",
    "4. Test Scores: Standardized test scores, like SAT or GRE, are often assumed to follow a normal distribution, facilitating score interpretation and percentile rankings.\n",
    "\n",
    "5. Financial Data: In finance, returns on assets or investments are often assumed to be normally distributed, enabling risk assessment and portfolio management.\n",
    "\n",
    "6. Natural Phenomena: Many naturally occurring phenomena, such as the distribution of rainfall, temperature variations, or noise in electronic systems, tend to follow a normal distribution.\n",
    "\n",
    "Parameters of the normal distribution and their relation to the shape of the distribution:\n",
    "\n",
    "The normal distribution is defined by two parameters: the mean (μ) and the standard deviation (σ). These parameters determine the location and the spread of the distribution, respectively.\n",
    "\n",
    "1. Mean (μ): The mean is the central value of the distribution and represents its location or the point of symmetry. It determines the peak of the bell-shaped curve. Shifting the mean to the left or right will move the entire distribution along the horizontal axis, without changing its shape.\n",
    "\n",
    "2. Standard Deviation (σ): The standard deviation measures the spread or dispersion of the distribution. A smaller standard deviation means the data points are closer to the mean, resulting in a narrower and taller curve. Conversely, a larger standard deviation means the data points are more spread out, leading to a wider and shorter curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f0ff6",
   "metadata": {},
   "source": [
    "##### Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47265111",
   "metadata": {},
   "source": [
    "The normal distribution is of paramount importance in statistics and data analysis for several reasons:\n",
    "\n",
    "1. Central Limit Theorem: One of the most significant reasons for the importance of the normal distribution is the Central Limit Theorem (CLT). The CLT states that the sum (or average) of a large number of independent and identically distributed random variables will tend to follow a normal distribution, regardless of the original distribution of the individual variables. This property makes the normal distribution a fundamental concept in statistical inference and hypothesis testing.\n",
    "\n",
    "2. Approximation of Other Distributions: In many real-world situations, data might not follow a perfect normal distribution. However, the normal distribution is often used as an approximation for other distributions, especially when sample sizes are large. This simplifies calculations and makes statistical methods more tractable.\n",
    "\n",
    "3. Parametric Modeling: The normal distribution is a parametric distribution, meaning it is completely defined by its mean and standard deviation. This simplicity makes it easier to work with in statistical modeling and estimation.\n",
    "\n",
    "4. Common Benchmark: The normal distribution serves as a common benchmark in statistical analyses. By understanding the properties of the normal distribution, researchers and analysts can compare and interpret data more effectively.\n",
    "\n",
    "Real-life examples of the Normal Distribution:\n",
    "\n",
    "1. Heights of Individuals: The heights of a large population tend to follow a normal distribution, with the majority of individuals falling close to the average height and fewer individuals being very tall or very short.\n",
    "\n",
    "2. Exam Scores: In educational testing, exam scores often approximate a normal distribution. The bulk of students usually score around the mean, with fewer students achieving extremely high or low scores.\n",
    "\n",
    "3. Errors in Measurements: Many measurement errors in scientific experiments are assumed to follow a normal distribution. This assumption is crucial for various statistical methods and hypothesis testing.\n",
    "\n",
    "4. IQ Scores: Intelligence quotient (IQ) scores are often modeled using a normal distribution. The mean IQ is typically set to 100, and the standard deviation is usually 15. This allows for easy comparison and interpretation of IQ scores.\n",
    "\n",
    "5. Reaction Times: In psychology and neuroscience, reaction times in response to stimuli often follow a normal distribution. Researchers use this distribution to analyze cognitive performance.\n",
    "\n",
    "6. Stock Returns: In finance, daily or monthly stock returns are often approximately normally distributed, though with fatter tails than a perfect normal distribution. This assumption is essential for various risk models and portfolio management strategies.\n",
    "\n",
    "7. Environmental Variables: Some environmental variables, such as temperature and precipitation, tend to approximate a normal distribution over long periods in a stable environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada0181",
   "metadata": {},
   "source": [
    "##### Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2f689",
   "metadata": {},
   "source": [
    "`Bernaulli Distribution:`\n",
    "\n",
    "The Bernoulli distribution is a discrete probability distribution that models a random experiment with only two possible outcomes: success (usually denoted as \"1\") or failure (usually denoted as \"0\"). It is named after the Swiss mathematician Jacob Bernoulli. The distribution is characterized by a single parameter, p, which represents the probability of success (or the probability of getting a value of 1).\n",
    "\n",
    "The probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\n",
    "P(X = x) = p^x * (1 - p)^(1 - x)\n",
    "\n",
    "where X is the random variable that takes values 0 or 1, and x is either 0 or 1.\n",
    "\n",
    "Example:\n",
    "A classic example of the Bernoulli distribution is modeling the outcome of a single coin toss. If we define \"1\" as getting a \"Heads\" and \"0\" as getting \"Tails,\" the probability of getting \"Heads\" in a fair coin toss is 0.5 (p = 0.5), and the probability of getting \"Tails\" is also 0.5.\n",
    "\n",
    "Difference between Bernoulli Distribution and Binomial Distribution:\n",
    "\n",
    "1. Number of Trials:\n",
    "- Bernoulli Distribution: It models a single random experiment with two possible outcomes (success or failure) in a single trial.\n",
    "- Binomial Distribution: It models the number of successes in a fixed number of independent Bernoulli trials (experiments) with the same probability of success (p) for each trial.\n",
    "\n",
    "2. Number of Possible Values:\n",
    "- Bernoulli Distribution: It has only two possible values, 0 and 1, representing failure and success, respectively.\n",
    "- Binomial Distribution: It has a range of possible values, starting from 0 (no successes) up to n (the number of trials).\n",
    "\n",
    "3. Probability Mass Function (PMF):\n",
    "- Bernoulli Distribution: The PMF for the Bernoulli distribution involves a single probability, p, for the success outcome (1).\n",
    "- Binomial Distribution: The PMF for the binomial distribution involves two parameters, n and p, where n is the number of trials, and p is the probability of success in each trial.\n",
    "\n",
    "4. Notation:\n",
    "- Bernoulli Distribution: It is denoted as B(p), where p represents the probability of success.\n",
    "- Binomial Distribution: It is denoted as B(n, p), where n represents the number of trials, and p represents the probability of success in each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2636ba",
   "metadata": {},
   "source": [
    "##### Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e0ee2",
   "metadata": {},
   "source": [
    "To find the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 will be greater than 60, we need to calculate the area under the normal curve to the right of the value 60.\n",
    "\n",
    "The formula to calculate this probability using the standard normal distribution (Z-distribution) is:\n",
    "\n",
    "P(X > 60) = 1 - P(X ≤ 60)\n",
    "\n",
    "where X is a random variable following a normal distribution with mean μ and standard deviation σ.\n",
    "\n",
    "To use the standard normal distribution, we need to transform the value 60 into a standard score (Z-score) using the formula:\n",
    "\n",
    "Z = (X - μ) / σ\n",
    "\n",
    "where X is the value we want to find the probability for, μ is the mean of the distribution, and σ is the standard deviation.\n",
    "\n",
    "Let's calculate the Z-score for X = 60:\n",
    "```\n",
    "Z = (60 - 50) / 10\n",
    "Z = 1\n",
    "```\n",
    "Now, we find the probability of Z being less than or equal to 1 using a standard normal distribution table or a calculator. The value of P(Z ≤ 1) is approximately 0.8413.\n",
    "\n",
    "Finally, we can find the probability of X being greater than 60:\n",
    "\n",
    "P(X > 60) = 1 - P(X ≤ 60)\n",
    "P(X > 60) = 1 - 0.8413\n",
    "P(X > 60) ≈ 0.1587\n",
    "\n",
    "So, the probability that a randomly selected observation from the given dataset will be greater than 60 is approximately 0.1587 or 15.87%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fcae86",
   "metadata": {},
   "source": [
    "##### Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a87c19",
   "metadata": {},
   "source": [
    "```\n",
    "The uniform distribution is a continuous probability distribution that models a situation where all outcomes within a certain range are equally likely. In other words, the probability of any value within the range is constant, resulting in a rectangular-shaped probability distribution. The uniform distribution is denoted as U(a, b), where 'a' and 'b' are the two endpoints of the range.\n",
    "\n",
    "The probability density function (PDF) of the uniform distribution is given by:\n",
    "\n",
    "f(x) = 1 / (b - a)   for a ≤ x ≤ b\n",
    "f(x) = 0            otherwise\n",
    "\n",
    "The cumulative distribution function (CDF) of the uniform distribution is given by:\n",
    "\n",
    "F(x) = 0             for x < a\n",
    "F(x) = (x - a) / (b - a)   for a ≤ x ≤ b\n",
    "F(x) = 1             for x > b\n",
    "\n",
    "Example:\n",
    "Consider a situation where you have a spinner with values ranging from 1 to 6, and each value is equally likely to occur when you give it a spin. This situation can be modeled using a uniform distribution.\n",
    "\n",
    "In this example, the random variable X represents the value obtained after spinning the spinner. The range of possible values is from 1 to 6, and each value has an equal chance of being selected.\n",
    "\n",
    "The probability density function (PDF) for this uniform distribution is:\n",
    "\n",
    "f(x) = 1 / (6 - 1) = 1/5    for 1 ≤ x ≤ 6\n",
    "f(x) = 0                   otherwise\n",
    "\n",
    "The cumulative distribution function (CDF) for this uniform distribution is:\n",
    "\n",
    "F(x) = 0                    for x < 1\n",
    "F(x) = (x - 1) / (6 - 1) = (x - 1) / 5    for 1 ≤ x ≤ 6\n",
    "F(x) = 1                    for x > 6\n",
    "\n",
    "This means that the probability of getting any value between 1 and 6 (inclusive) is 1/5, and the probability of getting a value less than 1 or greater than 6 is 0.\n",
    "\n",
    "The uniform distribution is useful in situations where we have a continuous range of equally likely outcomes, such as when selecting a random number from a uniform distribution between certain limits or in simulation and modeling scenarios where we want to simulate random events with equal probability over a specific range.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e62cb",
   "metadata": {},
   "source": [
    "##### Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c12a75",
   "metadata": {},
   "source": [
    "The z-score, also known as the standard score or standardization score, is a dimensionless quantity that measures the number of standard deviations a data point is from the mean of a dataset. It is a way to standardize or normalize data, allowing for meaningful comparisons and analysis across different datasets.\n",
    "\n",
    "Mathematically, the z-score of a data point X in a dataset with mean μ and standard deviation σ is calculated using the following formula:\n",
    "\n",
    "z = (X - μ) / σ\n",
    "\n",
    "where:\n",
    "- X is the data point for which we want to find the z-score.\n",
    "- μ is the mean of the dataset.\n",
    "- σ is the standard deviation of the dataset.\n",
    "\n",
    "The z-score indicates how many standard deviations a particular data point is above or below the mean. A positive z-score means the data point is above the mean, while a negative z-score means it is below the mean.\n",
    "\n",
    "Importance of the z-score:\n",
    "\n",
    "1. Standardization: The z-score standardizes data, allowing comparisons between data points from different datasets with different scales and units. It transforms data to a common scale, making it easier to interpret and analyze.\n",
    "\n",
    "2. Outlier Detection: The z-score is used to identify outliers, which are data points that deviate significantly from the mean. Outliers have extreme z-scores (either very large positive or negative values), indicating they are far from the mean and may be influential or erroneous observations.\n",
    "\n",
    "3. Probability Calculation: The z-score is crucial in calculating probabilities using the standard normal distribution (Z-distribution). When data is approximately normally distributed, the z-score helps determine the probability of observing a value or range of values.\n",
    "\n",
    "4. Hypothesis Testing: In hypothesis testing, the z-score is used to compare sample statistics with population parameters. It aids in determining whether a sample is significantly different from the population mean or whether there is a statistically significant effect.\n",
    "\n",
    "5. Data Transformation: The z-score transformation is applied in various statistical techniques and machine learning algorithms to improve the performance and interpretability of models. Standardizing data can enhance convergence in optimization algorithms and make coefficients comparable.\n",
    "\n",
    "6. Percentiles and Rankings: The z-score is used to convert raw data into percentiles and rank observations relative to the dataset's mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a938dd",
   "metadata": {},
   "source": [
    "##### Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419b502",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics. It states that when independent and identically distributed random variables are added together, their sum (or average) tends to follow a normal distribution, regardless of the distribution of the individual variables. This is true even if the individual variables do not follow a normal distribution themselves.\n",
    "\n",
    "The Central Limit Theorem is essential because it has several significant implications and applications in statistics:\n",
    "\n",
    "1. Normal Approximation: The CLT allows us to approximate the distribution of a sample mean (or sum) from any population, regardless of its original distribution, to a normal distribution. This is particularly useful when dealing with large sample sizes, where the normal approximation becomes more accurate.\n",
    "\n",
    "2. Sampling Distributions: The CLT is the basis for understanding sampling distributions. It tells us that if we repeatedly draw samples of the same size from a population, the distribution of the sample means will converge to a normal distribution, with the mean of the sample means equal to the population mean.\n",
    "\n",
    "3. Inference: The CLT is the foundation for many statistical inference techniques. For example, when we want to make inferences about a population parameter (such as the population mean) based on a sample, we use the CLT to determine the properties of the sampling distribution of the sample mean, which allows us to make statements about the population.\n",
    "\n",
    "4. Hypothesis Testing: The CLT is crucial in hypothesis testing. It allows us to compute the standard error of the sample mean, which is used to calculate test statistics and p-values to test hypotheses about population parameters.\n",
    "\n",
    "5. Confidence Intervals: The CLT enables us to construct confidence intervals around sample estimates, such as the sample mean. These intervals provide a range of values within which we are confident the population parameter lies.\n",
    "\n",
    "6. Decision Making: The CLT is extensively used in decision-making processes, such as quality control, market research, and public opinion polling. It allows us to make more accurate predictions and decisions based on sample data.\n",
    "\n",
    "7. Real-world Applications: The CLT is widely used in various fields, such as finance, economics, engineering, social sciences, and more, to model and analyze data that may not follow a normal distribution.\n",
    "\n",
    "In summary, the Central Limit Theorem is a fundamental statistical theorem that plays a central role in many statistical applications and inference. It allows us to make important conclusions about populations based on sample data and provides a bridge between probability theory and practical data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4e9a7",
   "metadata": {},
   "source": [
    "##### Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37494ee",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a powerful concept in statistics, but it relies on certain assumptions to hold true. The assumptions of the Central Limit Theorem are as follows:\n",
    "\n",
    "1. Independent and Identically Distributed (IID) Samples:\n",
    "The individual observations in the sample must be independent of each other, meaning that the value of one observation does not influence the value of another. Additionally, each observation should be drawn from the same underlying population, and the probability distribution of the population should have a finite variance.\n",
    "\n",
    "2. Sufficient Sample Size:\n",
    "The CLT becomes more reliable and accurate as the sample size increases. In general, a sample size of 30 or more is often considered sufficient for the CLT to hold reasonably well. However, for some distributions with heavy tails or high skewness, a larger sample size might be necessary.\n",
    "\n",
    "3. Finite Population Variance:\n",
    "The population from which the samples are drawn must have a finite variance. In practical terms, this means that the population cannot have extremely heavy tails or infinite variance.\n",
    "\n",
    "4. No Multimodality:\n",
    "The population distribution should not have multiple modes (peaks). If the population distribution is highly multimodal, the CLT may not apply.\n",
    "\n",
    "5. Random Sampling:\n",
    "The samples should be obtained through random sampling, meaning that every individual in the population has an equal chance of being selected in the sample. Non-random or biased sampling methods might violate the assumptions of the CLT.\n",
    "\n",
    "It is important to note that while the Central Limit Theorem is a powerful tool, violating these assumptions can lead to inaccurate results and conclusions. Therefore, it is essential to understand the data and the underlying population before applying the CLT. In some cases, alternative methods, such as bootstrapping or other distribution-free techniques, may be more appropriate if the assumptions of the CLT are not met."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
